# -*- coding: utf-8 -*-
"""Final_code_ToShow_Miami_SUMO_DataSet_MLP_SVM_KNN_Driver_Behavior.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f7m2pL-_wD_NSYu65J8PEQab9nvhISAu
"""

import torch

import torch
import torch.nn as nn
import torch.optim as optim
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

"""### **Set up the dataset**"""

import pandas as pd
import numpy as np

#This code groups the DataFrame by 'VehicleID' and iterates through each group separately, calculating the cumulative sum for each group.
def calculate_cumulative_total_per_vehicle(df):
     # Group by 'VehicleID' and iterate through groups
    for _, group_df in df.groupby('VehicleID'):
        cumulative_sum = 0

        for index, row in group_df.iterrows():
            cumulative_sum += row['Total']
            df.at[index, 'Total'] = cumulative_sum

    return df



def calculate_cumulative_total_per_vehicle(df, warning_features):
    result_df = df.copy()

    for warning in warning_features:
        # Group by 'VehicleID' and iterate through groups
        for _, group_df in df.groupby('VehicleID'):
            # Sort the group by 'Distance_driven' in descending order
            #group_df = group_df.sort_values(by='Distance_driven', ascending=False)


            # Calculate cumulative sum for the warning feature
            cumulative_sum = group_df[warning].cumsum()

            # Update the corresponding column in the result DataFrame
            result_df.loc[group_df.index, warning] = cumulative_sum

    return result_df

# Example usage

dataset_800V_Warning = '/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/800V_DS_Separated_worning_V2.csv'
dataset_800V_inVehicl= '/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/800V_DS_Export_dataframe_V2.csv'


df_800V_Warning = pd.read_csv(dataset_800V_Warning) ## df
df_800V_inVehicl = pd.read_csv(dataset_800V_inVehicl) ## df1

#compute the total warning
columns_to_sum = ['Speed_respect', 'secure_dist', 'TTC_respect', 'safe_dist', 'Emergency_Brake']
to_Sum_Up = ['Speed_respect', 'secure_dist', 'TTC_respect', 'safe_dist', 'Emergency_Brake', 'Total']
df_800V_Warning['Total'] = df_800V_Warning[columns_to_sum].sum(axis=1)
df_800V_Warning = calculate_cumulative_total_per_vehicle (df_800V_Warning,to_Sum_Up)
df_800V_Warning['Distance_driven']=df_800V_Warning['Distance_driven'].astype(int)


df_800V_Warning.to_csv('/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/Output_800V_DS_Separated_worning.csv', index=False)



dataset_1800V_Warning = '/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/1800V_DS_Separated_worning_V1.csv'
dataset_1800V_inVehicl = '/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/1800V_DS_Export_dataframe_V1.csv'

df_1800V_Warning = pd.read_csv(dataset_1800V_Warning)        ## Database2
df_1800V_inVehicl = pd.read_csv(dataset_1800V_inVehicl)  ## Database

#compute the total warning
columns_to_sum = ['Speed_respect', 'secure_dist', 'TTC_respect', 'safe_dist', 'Emergency_Brake']
to_Sum_Up = ['Speed_respect', 'secure_dist', 'TTC_respect', 'safe_dist', 'Emergency_Brake', 'Total']
df_1800V_Warning['Total'] = df_1800V_Warning[columns_to_sum].sum(axis=1)
df_1800V_Warning = calculate_cumulative_total_per_vehicle(df_1800V_Warning,to_Sum_Up )
df_1800V_Warning['Distance_driven']=df_1800V_Warning['Distance_driven'].astype(int)

df_1800V_Warning.to_csv('/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/Output_1800V_DS_Separated_worning.csv', index=False)

df= df_1800V_Warning ################################


print(df_1800V_inVehicl.shape)
print(df_800V_Warning.shape)
print(df_800V_Warning)
print(df_1800V_inVehicl)

# Example usage

dataset_800V_Warning = '/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/Output_800V_DS_Separated_worning.csv'
dataset_800V_inVehicl= '/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/800V_DS_Export_dataframe_V2.csv'
df_800V_Warning = pd.read_csv(dataset_800V_Warning) ## df
df_800V_inVehicl = pd.read_csv(dataset_800V_inVehicl) ## df1



dataset_1800V_Warning = '/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/Output_1800V_DS_Separated_worning.csv'
dataset_1800V_inVehicl = '/content/drive/MyDrive/Dataset_Driver_Behavior/Sumo_dataset_V1/1800V_DS_Export_dataframe_V1.csv'

#df = df.sort_values(by=[Label,'VehicleID','Total'],ascending=False)


df_1800V_Warning = pd.read_csv(dataset_1800V_Warning)        ## Database2
df_1800V_inVehicl = pd.read_csv(dataset_1800V_inVehicl)  ## Database

df= df_1800V_Warning ################################


print(df_1800V_Warning.shape)
print(df_800V_Warning.shape)
print(df_800V_Warning)
print(df_1800V_Warning)

"""
### **Grouper selon les steps**

"""

def Sum_x_step(df, features_to_keep ,features_to_sum,steps):

    # Round down the "Step" column to the nearest multiple of 5??????
    df["Step"] = df["Step"].apply(lambda n: ((n // steps) + 1) * steps)

    # Group by "VehicleID" and "Step" and calculate the sum for specified features
    features = features_to_sum
    #grouped_df = df.groupby(["VehicleID", "Step"])[features].sum()

    #grouped_df = df.groupby(["VehicleID", "Step"]).agg({"Distance_driven": "first", **{feature: "sum" for feature in features}})
    grouped_df = df.groupby(["VehicleID", "Step"]).agg({**{feature: "max" for feature in features_to_keep},
                                                        **{feature: "sum" for feature in features_to_sum}})



    return grouped_df

Feature_to_sum= ['Speed_respect', 'secure_dist', 'TTC_respect', 'safe_dist', 'Emergency_Brake', "Total" ]
features = ['Distance_driven', 'Speed_respect', "secure_dist", "TTC_respect", "safe_dist", "Emergency_Brake", "Total"]
features_to_keep = ['Distance_driven', 'Label']

# Groupe df by 50 step
df =  Sum_x_step(df,features_to_keep, Feature_to_sum,50)


# Groupe df_800V_Warning by 50 step
df_800V_Warning =  Sum_x_step(df_800V_Warning,features_to_keep, Feature_to_sum,50)


print(df.shape)
print(df_800V_Warning.shape)
print(df_800V_Warning)
print(df)

"""### **Preparation les donner pour ML algorithms**"""

X = df[features]
Y = df['Label']



x= df_800V_Warning[features]
y = df_800V_Warning['Label']



X_train= X
X_test= x
y_train= Y
y_test = y


y_train.replace(['Slow', 'Normal','Dangerous'],[-1, 0, 1], inplace=True)
y_test.replace(['Slow', 'Normal','Dangerous'],[-1, 0, 1], inplace=True)

"""Loading the Iris dataset

### **GBDT classifier**
"""

from sklearn.ensemble import GradientBoostingClassifier
clf1 = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0, max_depth=8,random_state=0)
clf1 = clf1.fit(X_train, y_train)

#print(X_train)
#print(y_test)
y_pred = clf1.predict(X_test)



print('Score, Training: ', clf1.score(X_train,y_train)) #accuracy score on training data
print('Score, Testing: ', clf1.score(X_test,y_test))

from sklearn.metrics import confusion_matrix


matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
print(' The confusion matrix: ')
print(matrix)

"""#### **KNN**"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import seaborn as sns

#Creating a NN classifier with K=3 and L1 distance (pour p=1)
knn = KNeighborsClassifier(n_neighbors=3)

#Fit tingour model
knn.fit(X_train, y_train)

y_pred = knn.predict(X_test)

print('Score, Training: ', knn.score(X_train,y_train)) #accuracy score on training data
print('Score, Testing: ', knn.score(X_test,y_test))


# accuracy = accuracy_score(y_test, y_pred)
# print(y_test)

#print("Accuracy:", accuracy)
print(y_pred)
from sklearn.metrics import confusion_matrix

matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
print(' The confusion matrix: ')
print(matrix)

"""### **MLP**"""

from sklearn.neural_network import MLPClassifier
#https://analyticsindiamag.com/a-beginners-guide-to-scikit-learns-mlpclassifier/#:~:text=MLPClassifier%20stands%20for%20Multi%2Dlayer,perform%20the%20task%20of%20classification.


X_val = X_test
Y_val = y_test
#print(Y_val)



#Initializing the MLPClassifier
#classifierMLP = MLPClassifier(hidden_layer_sizes=(4,8,4), max_iter=3,activation = 'relu',solver='adam',random_state=1)
classifierMLP = MLPClassifier(hidden_layer_sizes=(4,8,4), max_iter=300, alpha=0.01, learning_rate_init=0.001, solver='adam', verbose=10,  random_state=21,tol=0.0000001)

#Fitting the training data to the network
classifierMLP.fit(X_train, y_train)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
#Predicting y for X_val
X=0
y_pred = classifierMLP.predict(X_test)
# print(y_pred)
# print(y_pred.shape)
# print(y_test)


#print the value
# num_labels_to_print =1759
# for label in y_test:
#    i = 0
#    if label != y_pred [i]:
#         X+=1
#         i+=1
# print(X)


#Printing the accuracy
print(accuracy_score(y_test, y_pred))

#print(classifierMLP.score(X_val, Y_val))
print('Score, Training: ', classifierMLP.score(X_train,y_train)) #accuracy score on training data
print('Score, Testing: ', classifierMLP.score(X_val, Y_val))


matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
print(' The confusion matrix: ')
print(matrix)

"""### **SVM classifier**

"""

#import SVC classifier
import numpy as np
from sklearn.linear_model import SGDClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

#Creating a SVM classifier with RBF Kernerl
svc_clf = make_pipeline(StandardScaler(), SGDClassifier(max_iter=1000, tol=1e-3))
# Fitting our model
svc_clf.fit(X_train, y_train)

# Making predictions and calculate the Acc
predictions = svc_clf.predict(X_test)
# print(predictions)


print('Score, Training: ', svc_clf.score(X_train,y_train)) #accuracy score on training data
print('Score, Testing: ', svc_clf.score(X_test, y_test))


from sklearn.metrics import confusion_matrix

matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)
print(' The confusion matrix: ')
print(matrix)